{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time_series_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D8WkLiiRHFVc"
      },
      "outputs": [],
      "source": [
        "# In this tutorial, we will see how to model time series in Python and, then,\n",
        "# how we can make predictions of future time series data.\n",
        "\n",
        "# First of all, upload the dataset to Colab (file AirPassengers.csv)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Also, install a package that we are going to need\n",
        "\n",
        "!pip install --user statsmodels -U"
      ],
      "metadata": {
        "id": "85ovLqVG98rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('AirPassengers.csv', sep=',')\n",
        "dataset['Month_year'] = pd.to_datetime(dataset['Month'])\n",
        "dataset['Month'] = dataset['Month_year'].dt.month\n",
        "dataset = dataset[['Month_year', 'Month', 'Passengers']]\n",
        "dataset = dataset.set_index('Month_year')\n",
        "\n",
        "display(dataset)"
      ],
      "metadata": {
        "id": "Bj91kNLIHIdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First of all, let us have look at the data\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(dataset.index, dataset['Passengers'])\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ySuVdbBIdc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The information at out disposal goes from 1949-01-01 to 1960-12-01\t\n",
        "# The dataset covers 144 months, and in fact we have 144 rows in it, one for each month\n",
        "\n",
        "# From the plot, it immediately comes out that the time series is clearly non-stationary\n",
        "# The data has a clear upward trend over time\n",
        "# Moreover, it shows seasonality effects: winter months have typically less passengers than summer ones\n",
        "# In addition, also observe that variability is increasing with time"
      ],
      "metadata": {
        "id": "p4giEHenItIU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us highlight the trend in our data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.plot(dataset.index, dataset['Passengers'])\n",
        "\n",
        "p = np.polyfit(range(len(dataset.index)), dataset['Passengers'], 1) # Least squares polynomial fit: as a result it gives us the parameters a and b, in the line equation y=ax+b\n",
        "print(\"p:\", p)\n",
        "plt.plot(dataset.index, [x*p[0] + p[1] for x in range(len(dataset.index))], color='r')\n",
        "\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iOHaPrCVKMIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As for the seasonality, we can get confirmation about our thoughts regarding the difference between winter and summer months \n",
        "# passenger traffic aggregating the data by month, and making some boxplots.\n",
        "# Indeed, summer months have more passenger traffic\n",
        "\n",
        "dataset.boxplot(by='Month', grid=False, figsize=(20,7))\n"
      ],
      "metadata": {
        "id": "HoVTgkauKsT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us try to decompose the series into trend, seasonality and residual\n",
        "# As we have seen, since in this case the seasonality effects depend on time,\n",
        "# we have to rely on a multiplicative model to do that, i.e., Value = Trend * Seasonality * Error\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "result_mul = seasonal_decompose(dataset['Passengers'], model='multiplicative')\n",
        "\n",
        "fig = result_mul.plot()\n",
        "fig.set_size_inches((15, 7))\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IzNRGe79OD7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let us make some forecasts!\n",
        "\n",
        "# The first step is making the time series stationary.\n",
        "# Differencing is a classical operation by which we can do that.\n",
        "# Differencing can help to stabilize the mean of the time series by removing changes in the level\n",
        "# of a time series, and so eliminating (or reducing) trend and seasonality effects.\n",
        "# Differencing is performed by subtracting the previous observation from the current observation.\n",
        "\n",
        "# Pandas already provides the diff() method that does exactly what we would like to do\n",
        "\n",
        "differenced_series = dataset['Passengers'].diff()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.plot(dataset.index, differenced_series)\n",
        "\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Of course, here we have lost the first value of the series, since it does not have any point wrt calculate its difference\n",
        "\n",
        "# Anyway, the result is clearly still not stationary... what happened here?"
      ],
      "metadata": {
        "id": "WH-L7t6jm79M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The reason is that, since our time series has a strong seasonality effect, to a given value of index \"i\", we should\n",
        "# subtract the value at index \"i-[length of the period]\", istead of the one at \"i-1\"\n",
        "\n",
        "# To get a confirmation of that, let us investigate the autocorrelation of the time series\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "pd.plotting.autocorrelation_plot(dataset['Passengers'])\n",
        "plt.xlim(0, 120) # let us show the result up to 120 lags, i.e., 10 years\n",
        "plt.xticks(range(0,120,6))\n",
        "plt.show()\n",
        "\n",
        "# Notice how the autocorrelation decreases with the increase of the lag value\n",
        "# Nevertheless, we can clearly see the seasonal peaks at lags 12, 24, 36, 48 \n",
        "# Thus, our period here is 12 (unsurprisingly, a year)"
      ],
      "metadata": {
        "id": "gf59121E2sdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we subtract from each value of the time series the value that was observed in the same season one year earlier\n",
        "\n",
        "differenced_series = dataset['Passengers'].diff(12) # here, we are not considering the previous value anymore for the subtraction, but the 12th predecessor (the step consumes the initial 12 values)\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.plot(dataset.index, differenced_series)\n",
        "\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Uhm, better... but the time series is still not stationary, there is still a clear upward trend here..."
      ],
      "metadata": {
        "id": "NdfGroJAzlM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us now apply classical differencing on the seasonally differenced time series\n",
        "\n",
        "differenced_series = differenced_series.diff()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.plot(dataset.index, differenced_series)\n",
        "\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Now, this is stationary!\n",
        "\n",
        "# Observe that differencing can be applied several times, until we reach a stationary time series\n",
        "\n",
        "# As a final note, observe that the differencing process is invertible.\n",
        "# The inverse difference is the cumulative sum of the first value of the original series and then subsequent differences"
      ],
      "metadata": {
        "id": "xSIvGEO_zRDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us turn to some prediction tasks\n",
        "# The first step is that of dividing our data into a training and a test set\n",
        "\n",
        "train = dataset[dataset.index < pd.to_datetime(\"1957-12\", format='%Y-%m')]\n",
        "test = dataset[dataset.index >= pd.to_datetime(\"1957-12\", format='%Y-%m')]\n",
        "\n",
        "\n",
        "# First of all, let us have look at the data\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(dataset.index, dataset['Passengers'])\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "plt.axvline(pd.to_datetime(\"1957-12\", format='%Y-%m'), color='r')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VKxzmrL63M1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our baseline will be provided by a rather naive methodology, i.e., as the prediction we take the last known value\n",
        "\n",
        "prediction = list(train['Passengers'][-1:])*len(test.index)\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(train.index, train['Passengers'], color='tab:blue', label='training_data')\n",
        "plt.plot(test.index, test['Passengers'], linestyle='--', color='tab:blue', label='ground_truth_test')\n",
        "plt.plot(test.index, prediction, color='tab:orange', label='prediction')\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"Root mean squared error:\", np.sqrt(mean_squared_error(test[\"Passengers\"], prediction)))"
      ],
      "metadata": {
        "id": "GJZ2IGde0Lcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will employ (S)ARIMA \n",
        "# ARIMA stands for Auto Regressive Integrated Moving Average.\n",
        "# The acronym indicates that a ARIMA model has three components, i.e.:\n",
        "# - Auto-Regressive: past time series points may impact its current and future values. Thus, ARIMA uses p past observations\n",
        "#                    to forecast current and future values. Auto-regression is employed, that is, the process of applying\n",
        "#                    regression on a variable considering past values of itself. In other words, the current value of the series\n",
        "#                    is determined as a linear combination of p past values (which may also be previous forecasts).\n",
        "# - Integrated: the forecasting method cannot be applied to non-stationary time series. Thus, differencing is applied to \n",
        "#               reduce trend and seasonality effects.\n",
        "# - Moving Average: also past noise in the series might influence its current and future values. For instance, a \"shock\" in a \n",
        "#                   stock market time series will persist in a smaller extent in the next days as well. Thus, rather than\n",
        "#                   using the past values of the forecast, the moving average model uses past forecast errors in a\n",
        "#                   regression-like fashion. Specifically, the result is a weighted moving average over past forecast errors.\n",
        "# Specifically, since our data is seasonal, we will employ SARIMA (Auto Regressive Integrated Moving Average)\n",
        "#\n",
        "#\n",
        "# We will rely on the function \"ARIMA of the package \"statsmodels\"\n",
        "#\n",
        "# The function call is as follows: ARIMA(ts, order=(0, 0, 0), seasonal_order=(0, 0, 0, 0))\n",
        "#  - ts: is the training time series\n",
        "#  - order expects three values (p, d, q):\n",
        "#      p: number of lag values to consider in the auto-regressive part  (e.g., p=3 means we will use the 3 previous values of\n",
        "#           our time series in the autoregressive portion of the calculation)\n",
        "#      d: number of differencing transformations applied to the time series to make it stationary (from previous experiments,\n",
        "#           we already know we need 1 seasonal differencing and 1 \"plain\" differencing)\n",
        "#      q: the size of the moving average window\n",
        "#  - seasonal order expects four values (p, d, q, s): p, d, q are as before but related to the seasonal part, while \"s\" is the length of the period\n",
        "#\n",
        "# Thus, ARIMA is capable of dealing with time series differencing on its own (as a difference with respect to ARMA)\n",
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "model = ARIMA(train['Passengers'], order=(3, 1, 0), seasonal_order=(2, 1, 0, 12)) # using original training data and instructing ARIMA how to perform differencing\n",
        "model = model.fit()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "caKOgejm1B2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And now let us use the model for prediction purposes\n",
        "\n",
        "prediction = model.forecast(37)\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(train.index, train['Passengers'], color='tab:blue', label='training_data')\n",
        "plt.plot(test.index, test['Passengers'], linestyle='--', color='tab:blue', label='ground_truth_test')\n",
        "plt.plot(test.index, prediction, color='tab:orange', label='prediction')\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Root mean squared error:\", np.sqrt(mean_squared_error(test[\"Passengers\"], prediction)))"
      ],
      "metadata": {
        "id": "F90FbPeg9qHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let us try a RandomForestRegressor to perform the prediction\n",
        "# This is a fundamentally different approach with respect to the previous one, since\n",
        "# this kind of model is not capable of performing extrapolation from the training set data, i.e.,\n",
        "# it is only capable of handling values that it has seen during the training phase.\n",
        "# Here, we will understand even better why stationarizing a time series is important\n",
        "\n",
        "\n",
        "n_lags = 3\n",
        "\n",
        "# The following function generates a dataset in the format required by the RandomForestRegressor\n",
        "\n",
        "def buildLaggedFeatures(input_series, lag=3):\n",
        "  result = np.zeros((len(input_series)-lag, lag+1))\n",
        "  for i in range(lag, len(input_series)):\n",
        "    result[i-lag] = [input_series[i-j] for j in range(lag, -1, -1)]\n",
        "  colnames = [\"lag_\" + str(i) for i in range(lag, 0, -1)]\n",
        "  return pd.DataFrame(result[:,:-1], columns=colnames), pd.DataFrame(result[:,-1], columns=[\"Label\"])\n",
        "\n",
        "\n",
        "X_train, y_train = buildLaggedFeatures(train['Passengers'], lag=n_lags)\n",
        "X_test, y_test = buildLaggedFeatures(test['Passengers'], lag=n_lags)\n",
        "\n",
        "\n",
        "display(X_train)\n",
        "display(y_train)\n",
        "\n",
        "display(train)"
      ],
      "metadata": {
        "id": "4pnbDjW_CAK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train.values, y_train)\n",
        "\n",
        "\n",
        "# Of course, when we perform the prediction, we start from the last known values of the training set\n",
        "# Then, each successive predictions will be based on the previous predictions\n",
        "# E.g.: [train_n-2, train_n-1, train_n] --> pred_0, [train_n-1, train_n, pred_0] --> pred_1, [train_n, pred_0, pred_1] --> pred_2, ...\n",
        "\n",
        "# input: list of training labels, number of lag values to consider, number of predictions to perform\n",
        "def rf_predict(train_labels, lag, num_preds, model_in):\n",
        "  predictors = train_labels[-lag:].values.flatten()\n",
        "  preds = []\n",
        "  for i in range(num_preds):\n",
        "    pred = model_in.predict(predictors.reshape(-1, lag))\n",
        "    preds.append(pred)\n",
        "    predictors = np.roll(predictors, -1)\n",
        "    predictors[-1] = pred\n",
        "  return preds\n",
        "\n",
        "\n",
        "\n",
        "# And now let us use the model for prediction purposes\n",
        "\n",
        "prediction = rf_predict(y_train, n_lags, 37, model)\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(train.index, train['Passengers'], color='tab:blue', label='training_data')\n",
        "plt.plot(test.index, test['Passengers'], linestyle='--', color='tab:blue', label='ground_truth_test')\n",
        "plt.plot(test.index, prediction, color='tab:orange', label='prediction')\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Root mean squared error:\", np.sqrt(mean_squared_error(test[\"Passengers\"], prediction)))\n",
        "\n",
        "\n",
        "# Note that RF cannot predict the future values very well\n",
        "# This is to be expected, as the training set provided no information about how to estimate them\n",
        "# Specificially we could not predict the increase in the trend and seasonality effect just based on the 3 previous lag values"
      ],
      "metadata": {
        "id": "2EGIBesLEHjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating training and test datasets introducing also the differencing operations\n",
        "\n",
        "differenced_series = dataset['Passengers'].diff(12) # differencing taking into account seasonality\n",
        "differenced_series = differenced_series.diff() # another step of differencing\n",
        "\n",
        "train_differenced = differenced_series[dataset.index < pd.to_datetime(\"1957-12\", format='%Y-%m')].dropna() # the first 13 values are undefined due to the differencing operation\n",
        "test_differenced = differenced_series[dataset.index >= pd.to_datetime(\"1957-12\", format='%Y-%m')]\n",
        "\n",
        "X_train, y_train = buildLaggedFeatures(train_differenced, lag=n_lags)\n",
        "X_test, y_test = buildLaggedFeatures(test_differenced, lag=n_lags)\n",
        "\n",
        "\n",
        "# Training model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train.values, y_train)\n",
        "\n",
        "\n",
        "# And now let us use the model for prediction purposes\n",
        "\n",
        "prediction = rf_predict(y_train, n_lags, 37, model)\n",
        "\n",
        "\n",
        "# Helper function that we are gointo to use to revert the differencing steps\n",
        "def diff_inv(train_vals, series_in, difflag=1):\n",
        "  whole_series = list(train_vals) + list(series_in)\n",
        "  for i in range(0, len(whole_series)):\n",
        "    if i - difflag >= 0:\n",
        "      whole_series[i] = whole_series[i] + whole_series[i-difflag]\n",
        "  return whole_series\n",
        "\n",
        "\n",
        "revert_diff = diff_inv(np.asarray(y_train).flatten(), np.asarray(prediction).flatten(), 1)\n",
        "revert_season = diff_inv(np.asarray(y_train).flatten(), revert_diff, 12)\n",
        "revert_initial_value = np.asarray(revert_season) + train['Passengers'][12]\n",
        "actual_prediction = revert_initial_value[-37:]\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(train.index, train['Passengers'], color='tab:blue', label='training_data')\n",
        "plt.plot(test.index, test['Passengers'], linestyle='--', color='tab:blue', label='ground_truth_test')\n",
        "plt.plot(test.index, actual_prediction, color='tab:orange', label='prediction')\n",
        "plt.title(\"Number of passengers by month\")\n",
        "plt.xticks(dataset.index[::3], rotation=90)\n",
        "plt.xlabel(\"Month_year\")\n",
        "plt.ylabel(\"Number of passengers\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Root mean squared error:\", np.sqrt(mean_squared_error(test_differenced.values, prediction)))"
      ],
      "metadata": {
        "id": "OAL-DkkeC5mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ppHHyah2eH4E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}